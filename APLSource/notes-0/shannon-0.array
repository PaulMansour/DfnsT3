'entropy ← ##.shannon string                 ⍝ Shannon entropy of message ⍵.⍞000D⍞000DFrom Gianfranco Alongi:  Shannon entropy is one of the most important metrics in⍞000Dinformation theory.  Entropy measures the uncertainty associated with  a  random⍞000Dvariable, i.e. the expected value of the information in the message.⍞000D⍞000DThe concept was introduced by Claude E. Shannon  in  the  paper  "A Mathematical⍞000DTheory of Communication" (1948). Shannon entropy allows us to estimate the aver-⍞000Dage minimum number of bits needed to encode a string of  symbols  based  on  the⍞000Dalphabet size and the frequency of the symbols.⍞000D⍞000DThe entropy of the message is its amount of uncertainty;  it  increases when the⍞000Dmessage is closer to random, and decreases when it is less random.⍞000D⍞000D    Entropy of X ≡ H(X) := - sigma_i P(X_i) log_2(P(X_i))⍞000D    where⍞000D        P(X_i) = frequency of X_i in X⍞000D⍞000DRef: https://en.wikipedia.org/wiki/Information_theory#Entropy⍞000D⍞000DExamples:⍞000D⍞000D   (shannon⍤1 , ⊣) 1+~(⍳10) ∘.> ⍳10⍞000D0            2 2 2 2 2 2 2 2 2 2⍞000D0.4689955936 1 2 2 2 2 2 2 2 2 2⍞000D0.7219280949 1 1 2 2 2 2 2 2 2 2⍞000D0.8812908992 1 1 1 2 2 2 2 2 2 2⍞000D0.9709505945 1 1 1 1 2 2 2 2 2 2⍞000D1            1 1 1 1 1 2 2 2 2 2⍞000D0.9709505945 1 1 1 1 1 1 2 2 2 2⍞000D0.8812908992 1 1 1 1 1 1 1 2 2 2⍞000D0.7219280949 1 1 1 1 1 1 1 1 2 2⍞000D0.4689955936 1 1 1 1 1 1 1 1 1 2⍞000D⍞000DSee also: Data_compression⍞000D⍞000DIndex: Entropy|information theory⍞000DIndex; Alongi G.|Shannon C.E.' 
